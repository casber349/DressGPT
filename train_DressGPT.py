import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np
import pandas as pd
import os
import copy

# å¼•å…¥ä½ çš„æ¨¡çµ„
from feature_utils import get_one_hot_tags
from model_arch import DressGPT

# 1. è¨­å®šè·¯å¾‘
CSV_PATH = "dress_dataset.csv"
EMBEDDINGS_PATH = "image_embeddings.pt"
MODEL_SAVE_DIR = "./DressGPT_models"
INFO_FILE_PATH = "./DressGPT_models/model_info.txt"
AUDIT_FILE_PATH = "./ensemble_validation_audit.csv"

# ç¢ºä¿å„²å­˜è³‡æ–™å¤¾å­˜åœ¨
if not os.path.exists(MODEL_SAVE_DIR):
    os.makedirs(MODEL_SAVE_DIR)

def load_and_prepare_data():
    df = pd.read_csv(CSV_PATH)
    df['id'] = df['id'].apply(lambda x: str(x).zfill(4))
    id_to_feat = torch.load(EMBEDDINGS_PATH)
    
    X_list = []
    y_list = []
    valid_indices = []
    
    print("ğŸ”„ æ­£åœ¨å°é½Šåœ–ç‰‡ç‰¹å¾µèˆ‡æ–‡å­—æ¨™ç±¤...")
    for idx, row in df.iterrows():
        img_id = row['id']
        if img_id in id_to_feat:
            img_feat = id_to_feat[img_id].to(torch.float32).flatten()
            tag_feat = get_one_hot_tags(row)
            combined_feat = torch.cat([img_feat, tag_feat]) 
            
            X_list.append(combined_feat)
            y_list.append(row['score'])
            valid_indices.append(idx)

    X = torch.stack(X_list)
    y = torch.tensor(y_list, dtype=torch.float32).view(-1, 1)
    valid_df = df.loc[valid_indices].reset_index(drop=True)
    return X, y, valid_df

# ==========================================
# æ ¸å¿ƒæ”¹å‹•ï¼šåˆ†å±¤åˆ†æ¡¶ (Stratified Bucketing)
# ==========================================
def get_anchored_split(df, n_splits=5):
    """
    å°‡ 0-10 åˆ†åˆ‡å‰²ç‚º 10 å€‹å€æ®µï¼Œç¢ºä¿æ¯å€‹ Fold çš„é©—è­‰é›†åœ¨å„å€æ®µçš„ä½”æ¯”å‡ç­‰
    """
    fold_ids = np.full(len(df), -1)
    
    # å®šç¾©æ¡¶å­ (0-1, 1-2, ..., 9-10)
    # ä½¿ç”¨ np.floor å°‡åˆ†æ•¸åˆ†é¡ (ä¾‹å¦‚ 9.45 åˆ†å±¬æ–¼ç¬¬ 9 æ¡¶)
    # 10 åˆ†æœƒè¢«åˆ†åˆ°ç¬¬ 10 æ¡¶ï¼Œæˆ‘å€‘å°‡å…¶ä½µå…¥ç¬¬ 9 æ¡¶
    df['bucket'] = df['score'].apply(lambda x: min(int(np.floor(x)), 9))
    
    print(f"âš“ åŸ·è¡Œå…¨å€æ®µåˆ†å±¤ï¼šæ­£åœ¨å°‡ 1000 ç­†è³‡æ–™å‡å‹»åˆ†é…è‡³ {n_splits} å€‹ Fold...")

    for bucket_val in range(10):
        # æŠ“å‡ºè©²åˆ†æ•¸æ®µçš„æ‰€æœ‰ç´¢å¼•
        bucket_indices = df[df['bucket'] == bucket_val].index.to_numpy()
        np.random.shuffle(bucket_indices)
        
        # å°‡è©²æ¡¶å­çš„ç´¢å¼•è¼ªæµåˆ†é…çµ¦å„å€‹ Fold
        for i, idx in enumerate(bucket_indices):
            fold_ids[idx] = i % n_splits
            
    # æª¢æŸ¥æ˜¯å¦æœ‰æ¼ç¶²ä¹‹é­š
    if -1 in fold_ids:
        remaining = np.where(fold_ids == -1)[0]
        for i, idx in enumerate(remaining):
            fold_ids[idx] = i % n_splits

    for i in range(n_splits):
        val_mask = (fold_ids == i)
        yield np.where(~val_mask)[0], np.where(val_mask)[0]

def z_weighted_mse_loss(preds, targets):
    z_scores = (targets - 5.0) / 1.5
    abs_z = torch.abs(z_scores)
    # V11 è¿½æ±‚æ³›åŒ–ä¿‚æ•¸
    reward_multiplier = 0.6
    penalty_multiplier = 0.9
    weights = torch.where(targets >= 5.0, 1.0 + reward_multiplier * abs_z, 1.0)
    weights = torch.where(targets < 5.0, 1.0 + penalty_multiplier * abs_z, weights)
    return (weights * (preds - targets) ** 2).mean()

def print_distribution_health_check(df):
    total = len(df)
    
    # 1. æ ¸å¿ƒå¹³åº¸å€ (4.00 ~ 6.00)
    mid_zone = df[(df['val_score'] >= 4.00) & (df['val_score'] <= 6.00)]
    mid_count = len(mid_zone)
    
    # 2. é«˜åˆ†å¤©å ‚å€ (>= 8.00)
    high_zone = df[df['val_score'] >= 8.00]
    high_count = len(high_zone)
    
    # 3. ä½åˆ†åœ°ç„å€ (<= 2.00)
    low_zone = df[df['val_score'] <= 2.00]
    low_count = len(low_zone)
    
    print("\n" + "="*40)
    print("ğŸš€ DressGPT V11 åˆ†ä½ˆå¥åº·æª¢æŸ¥")
    print("="*40)
    print(f"1. å¹³åº¸å€ (4.0-6.0): {mid_count:>4} äºº (é è¨ˆ ~500) | ä½”æ¯”: {mid_count/total:.1%}")
    print(f"2. é«˜åˆ†å€ (>= 8.0): {high_count:>4} äºº (é è¨ˆ ~20)  | ä½”æ¯”: {high_count/total:.1%}")
    print(f"3. ä½åˆ†å€ (<= 2.0): {low_count:>4} äºº (é è¨ˆ ~20)  | ä½”æ¯”: {low_count/total:.1%}")
    print("="*40)
    
    # ç•°å¸¸è­¦å ±é‚è¼¯
    if high_count < 5:
        print("âš ï¸ è­¦å ±ï¼šé«˜åˆ†å€äººæ•¸å¤ªå°‘ï¼æ¨¡å‹å¯èƒ½éæ–¼ä¿å®ˆï¼ŒåŠ åˆ†åŠ çš„ä¸å¤ é‡ã€‚")
    if low_count < 5:
        print("âš ï¸ è­¦å ±ï¼šä½åˆ†å€äººæ•¸å¤ªå°‘ï¼æ¨¡å‹å¯èƒ½éæ–¼æ…ˆæ‚²ï¼Œæ‰£åˆ†æ‰£çš„ä¸å¤ é‡ã€‚")
    if mid_count > total * 0.7:
        print("âš ï¸ è­¦å ±ï¼šæ¨¡å‹çµ¦åˆ†å¤ªå¾€ä¸­é–“åã€‚")
    if mid_count < total * 0.3:
        print("âš ï¸ è­¦å ±ï¼šæ¨¡å‹çµ¦åˆ†å¤ªå¾€å…©é‚Šåã€‚")

# ==========================================
# ä¸»è¨“ç·´æµç¨‹
# ==========================================

X, y, valid_df = load_and_prepare_data()
splitter = get_anchored_split(valid_df, n_splits=5)
fold_stats = [] 
all_oof_results = [] # å„²å­˜æ‰€æœ‰ç›²æ¸¬çµæœ

print(f"ğŸš€ é–‹å§‹æ•´åˆå¼ V11 è¨“ç·´ (å«å…¨ç›²æ¸¬ç¨½æ ¸)...")

for fold, (t_idx, v_idx) in enumerate(splitter):
    X_t, X_v, y_t, y_v = X[t_idx], X[v_idx], y[t_idx], y[v_idx]
    model = DressGPT()
    # åŠ å…¥ Weight Decay å¼·åˆ¶æ³›åŒ–
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    
    best_v_loss = float('inf') 
    best_metrics = {}
    best_model_wts = None

    for epoch in range(1, 1001):
        model.train()
        optimizer.zero_grad()
        loss = z_weighted_mse_loss(model(X_t), y_t)
        loss.backward()
        optimizer.step()
        
        if epoch % 5 == 0:
            model.eval()
            with torch.no_grad():
                p_t, p_v = model(X_t), model(X_v)
                curr_v_loss = z_weighted_mse_loss(p_v, y_v).item()

                if curr_v_loss < best_v_loss:
                    best_v_loss = curr_v_loss
                    best_model_wts = copy.deepcopy(model.state_dict())
                    p_t_np, p_v_np = p_t.numpy(), p_v.numpy()
                    y_t_np, y_v_np = y_t.numpy(), y_v.numpy()
                    
                    best_metrics = {
                        'train_loss': z_weighted_mse_loss(p_t, y_t).item(),
                        'val_loss': curr_v_loss,
                        'train_rmse': np.sqrt(mean_squared_error(y_t_np, p_t_np)),
                        'val_rmse': np.sqrt(mean_squared_error(y_v_np, p_v_np)),
                        'train_r2': r2_score(y_t_np, p_t_np),
                        'val_r2': r2_score(y_v_np, p_v_np),
                        'best_epoch': epoch
                    }
                    patience = 0
                else:
                    patience += 1
            if patience >= 5: break

    # å„²å­˜èˆ‡è¨˜éŒ„
    torch.save(best_model_wts, os.path.join(MODEL_SAVE_DIR, f"fold{fold+1}.pth"))
    fold_stats.append({"fold": fold+1, **best_metrics})
    
    # ç´€éŒ„æ®˜å·® (Residuals)
    model.load_state_dict(best_model_wts)
    model.eval()
    with torch.no_grad():
        v_preds = model(X_v).flatten().numpy()
        v_reals = y_v.flatten().numpy()
        v_ids = valid_df.iloc[v_idx]['id'].values
        for i in range(len(v_ids)):
            diff = abs(round((float(v_reals[i]) - float(v_preds[i])), 2))
            all_oof_results.append({
                'id': v_ids[i], 'real': round(float(v_reals[i]), 2),
                'val_score': round(float(v_preds[i]), 2),
                'diff': diff, 'which_fold': fold + 1
            })
    print(f"âœ… Fold {fold+1} å®Œæˆ. T-loss: {best_metrics['train_loss']:.4f}, V-loss: {best_metrics['val_loss']:.4f}")

# 3. è¼¸å‡º model_info.txt (ä¸åˆªæ¸›ä»»ä½•æ¬„ä½)
print(f"ğŸ“ æ­£åœ¨å¯«å…¥è¨“ç·´å ±å‘Šè‡³ {INFO_FILE_PATH}...")

avg_t_loss = np.mean([s['train_loss'] for s in fold_stats])
avg_v_loss = np.mean([s['val_loss'] for s in fold_stats])
avg_t_rmse = np.mean([s['train_rmse'] for s in fold_stats])
avg_v_rmse = np.mean([s['val_rmse'] for s in fold_stats])
avg_t_r2 = np.mean([s['train_r2'] for s in fold_stats])
avg_v_r2 = np.mean([s['val_r2'] for s in fold_stats])

with open(INFO_FILE_PATH, "w", encoding="utf-8") as f:
    f.write("=== DressGPT v11 Anchored Split Training Report ===\n\n")
    header = f"{'Fold':<5} | {'Epoch':<6} | {'T-Loss':<8} | {'V-Loss':<8} | {'T-RMSE':<8} | {'V-RMSE':<8} | {'T-R2':<8} | {'V-R2':<8}\n"
    f.write(header)
    f.write("-" * len(header) + "\n")
    
    for s in fold_stats:
        line = (f"{s['fold']:<5} | {s['best_epoch']:<6} | "
                f"{s['train_loss']:.4f}   | {s['val_loss']:.4f}   | "
                f"{s['train_rmse']:.4f}   | {s['val_rmse']:.4f}   | "
                f"{s['train_r2']:.4f}   | {s['val_r2']:.4f}\n")
        f.write(line)
    
    f.write("-" * len(header) + "\n")
    avg_line = (f"{'AVG':<5} | {'-':<6} | "
                f"{avg_t_loss:.4f}   | {avg_v_loss:.4f}   | "
                f"{avg_t_rmse:.4f}   | {avg_v_rmse:.4f}   | "
                f"{avg_t_r2:.4f}   | {avg_v_r2:.4f}\n")
    f.write(avg_line)

# 4. è¼¸å‡º ensemble_validation_audit.csv
audit_df = pd.DataFrame(all_oof_results).sort_values(by='id')

# å‘¼å«çµ±è¨ˆå‡½å¼
print_distribution_health_check(audit_df)

audit_df.to_csv(AUDIT_FILE_PATH, index=False)
print(f"\nâœ¨ æ•´åˆæˆåŠŸï¼\n1. è¨“ç·´å ±å‘Šå·²æ›´æ–°ï¼š{INFO_FILE_PATH}\n2. å…¨ç›²æ¸¬ç¨½æ ¸è¡¨å·²ç”¢å‡ºï¼š{AUDIT_FILE_PATH}")